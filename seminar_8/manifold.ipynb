{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Problem statement\n",
    "1. Random Projections\n",
    "1. PCA\n",
    "1. Truncated SVD\n",
    "1. NMF\n",
    "1. Manifold learning\n",
    "1. TSNE\n",
    "1. Autoencoders\n",
    "1. Denoising Autoencoders\n",
    "1. Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Problem Statement\n",
    "\n",
    "\n",
    "Given a set of points $X \\in R^{NxD}$, find a map into a lower dimension subspace $f: R^D \\rightarrow R^d$ $d  << D$\n",
    "\n",
    "Depending of the map function there are:  \n",
    "\n",
    "1. Linear methods: PCA, SVD, NMF, etc..\n",
    "1. Nonlinear methods\n",
    "\n",
    "Sometimes, feature selection is considered as dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Random Projections\n",
    "\n",
    "<img src=\"images/proj.png\" style=\"height:300px\">\n",
    "\n",
    "$$ f(X) = X S $$\n",
    "where $ S \\in R^d$\n",
    "\n",
    "A common example is a gaussian random projection:\n",
    "1. S is sampled from a normal gaussian distribution\n",
    "1. each row has unit norm $ ||S_{i,:} || = 1$\n",
    "1. rows are pairwise orthogonal $ S_{i,:}^T S_{j,:} = 0 $ iff $i \\neq j$\n",
    "\n",
    "\n",
    "\n",
    "Johnson - Lindenstrauss Lemma:   \n",
    "States, that a set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved.  \n",
    "\n",
    "Given $0 < \\epsilon  < 1$, a set of points $X \\in R^D$, number $d > \\frac {8 \\log(m)} {\\epsilon^2}$, there exists a linear map $f: R^D \\rightarrow R^d$ such that:  \n",
    "$$ (1-\\epsilon) ||u - v||^2 \\leq ||f(u) - f(v)||^2 \\leq (1+\\epsilon) ||u - v||^2$$\n",
    "\n",
    "There exists a set of points of size $m$ that needs dimension  \n",
    "$O ( \\frac {\\log(m)} {\\epsilon^2} )$ in order to preserve the distances between all pair of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 PCA\n",
    "\n",
    "Preserve more information while reducing feature dimensions. Here we make assumption that we measure quantity of information by variance. More variable the feature is, more information it contains.\n",
    "\n",
    "Learning objective:\n",
    "$$ ||X U - \\hat X_k U_k||_F \\rightarrow \\min_{U}$$, where  $rank(\\hat X_k) = k$ and $U_k$ contains first k column vectors of $U$. \n",
    "\n",
    "\n",
    "General scheme for PCA:\n",
    "\n",
    "1. center data $\\bar X = X - mean(X)$  \n",
    "1. build covariance matrix $cov = \\bar X \\bar X^T$  \n",
    "1. make low-rank approximation for covariance matrix $X \\approx \\hat X_k = U_k \\Sigma_k U_k^T$ by eigendecomposition\n",
    "1. sort eigenvalues in descending order by their absolute value\n",
    "1. select top-k eigenvalues and project data onto corresponding eigenvectors\n",
    "\n",
    "Learned mapping:\n",
    "$$f(X) = X U_k$$\n",
    "\n",
    "\n",
    "Properties:\n",
    "1. PCA basis vectors creates an uncorrelated orthogonal basis set.  \n",
    "1. PCA is sensitive to feature scaling\n",
    "1. variance explained ratio of  j-th principal component = $\\frac {|\\Sigma_jj|} / {trace(|S|)} $  \n",
    "\n",
    "<img src=\"images/pca.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Truncated SVD\n",
    "\n",
    "Learning objective:  \n",
    "$$ || X - \\hat X_k||_F \\rightarrow min$$\n",
    "subject to $rank(X_k) = k$.  \n",
    "\n",
    "which has the exact solution given by SVD decomposition\n",
    "$$ X \\approx \\hat X_k = U_k \\Sigma_k V^T$$\n",
    "\n",
    "Learned mapping:  \n",
    "$ \\hat X = X V_k$\n",
    "\n",
    "Properties:\n",
    "1. Do not use covariance matrix\n",
    "1. Do not center data\n",
    "1. Preferable over PCA for sparse features\n",
    "\n",
    "<img src=\"images/svd.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 NMF \n",
    "\n",
    "Learnable objective:\n",
    "\n",
    "$$ || X - WH ||_F \\rightarrow \\min_{W, H}$$\n",
    "subject to $X >= 0$, $W >= 0$ and $H >=0 0$.\n",
    "\n",
    "$$ X \\approx \\hat X_k = W H $$\n",
    "\n",
    "Regularized with ElasticNet:  \n",
    "$ || X - WH ||_F + \\gamma *( \\alpha(||W||_1 + ||H||_1)  + \\frac {1 - \\alpha} 2 ( ||W||_2  + ||H||_2)) \\rightarrow \\min_{W, H} $\n",
    "\n",
    "Properties:\n",
    "1. use for tf-idf features\n",
    "1. more interpretable solution (topic modeling)\n",
    "\n",
    "<img src=\"images/nmf.jpg\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Manifold learning\n",
    "\n",
    "Data lies on a low-dimensional non-linear manifold in a high-dimensional space. In other words, features are connected by some non-linear function.\n",
    "\n",
    "\n",
    "<img src=\"images/manifold1.png\" style=\"height:300px\">\n",
    "\n",
    "<img src=\"images/manifold2.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 TSNE\n",
    "\n",
    "## t-distributed stochastic neighbor embedding\n",
    "\n",
    "\n",
    "Algorithm:  \n",
    "1. Compute probability that a sample $x_i$ would peak $x_j$ as its neighbour  \n",
    "$$ p(j | i) = \\frac {1} {Z} \\exp (- || x_i - x_j||^2 / 2 \\sigma_i^2) $$  \n",
    "$Z = \\sum_{k \\neq i} \\exp (- || x_i - x_k||^2 / 2 \\sigma_i^2) $ - normalization factor\n",
    "\n",
    "2. probability that $x_i$ and $x_j$ are neighbors:  \n",
    "$$ p(i,j) = \\frac {p(i | j)  + p(j | i)} 2$$  \n",
    "but $p(i,i) = 0$\n",
    "\n",
    "3. introduce map $Y \\in R^d$, for wich probabolity that $y_i$ and $y_j$ are neighbors:   \n",
    "$$ q(i, j) = \\frac 1 Z (1 + || y_i - y_j||^2 )^{-1}$$\n",
    "$Z = \\sum_{k \\neq i} (1 + || y_i - y_k||^2 )^{-1} $ - normalization factor\n",
    "\n",
    "4. Learning objective = minimize Kullbackâ€“Leibler divergence (distance between distributions):\n",
    "$$ KL(P || Q) = \\sum_{i \\neq j} p(i,j) \\log \\frac {p(i,j)} {q(i,j)} \\rightarrow \\min_{Y} $$\n",
    "\n",
    "\n",
    "# demo here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Autoencoders\n",
    "\n",
    "Autoencoders are feed-forward neural networks, which contains encoder and decoder part. All training methods applicable to feed-forward NN, also can be applied to autoencoders. But unlike feed-forward NN, autoencoders are unsipervised models.\n",
    "\n",
    "Let $E$ be encoder, $D$ - decoder, $L$  - some loss function.\n",
    "\n",
    "Than, in the most simple case, the goal is to learn an efficient feature represetation by directing information through a bottleneck and predicting sample itself: \n",
    "$$ L(X, D(E(X))) \\rightarrow \\min_{D, E}$$\n",
    "\n",
    "<img src=\"images/autoencoder.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Denoising autoencoders\n",
    "\n",
    "Denoising autoencoders try to reconstruct a sample from a noisy one:\n",
    "\n",
    "$$ \\bar X = X + \\epsilon $$,\n",
    "usually we use white noise $\\epsilon ~ N(0,1)$\n",
    "\n",
    "Learning objective:\n",
    "$$ L(X, D(E(\\bar X))) \\rightarrow \\min_{D, E}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Sparse autoencoders\n",
    "\n",
    "Sparse autoencoders use another definition of efficiency: good representation should be sparse, so particular sample phenotype will depend only on a small number of features (ideally, 1 feature). It can be achieved by imposing $L_1$ regularization on feature representation.\n",
    "\n",
    "$$ Z = E(X)$$\n",
    "$$ L(X, D(Z)) + \\alpha ||Z||_1 \\rightarrow \\min_{D, E} $$\n",
    "\n",
    "<img src=\"images/sparse.jpg\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "There are other autoencoder models not discussed here, e.g. variational autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
